{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model\n",
    "import sklearn.cross_validation\n",
    "import sklearn.metrics\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "import util.bq_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load up some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticating with BigQuery and GCS...\n",
      "Go to the following link in your browser:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fbigquery&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&client_id=124072386181-qqedvnl36ver0khc3pmqbh4bevlh58qd.apps.googleusercontent.com&access_type=offline\n",
      "\n",
      "Enter verification code: 4/NwX_SdAbLpRgFUQUHaam-txLfdM0O7SQypvr3juM33M\n",
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "bq, gcs = util.bq_util.get_authed_clients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bq.tables().list(projectId=\"khanacademy.org:deductive-jet-827\", datasetId=\"recommendations\").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'amy/rec_training000000000000.csv']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = \"ka_users/amy/\"\n",
    "filename = \"rec_training000000000000.csv\"\n",
    "util.bq_util.list_files_in_gcs_bucket(gcs, os.path.join(bucket, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "util.bq_util.download_files_from_gcs(gcs, os.path.join(bucket, filename))\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define preprocessing + modeling functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_processing(userdf):\n",
    "    \"\"\"Preprocess dataframe for a single user\"\"\"\n",
    "    DV = [\"engaged\"] # DV = did they engage with the content\n",
    "    features = [\"kind\", \n",
    "                \"title\", \n",
    "                \"domain\", \n",
    "                \"subject\", \n",
    "                \"topic\", \n",
    "                \"tutorial\", \n",
    "                \"num_learners_per_day\", \n",
    "                \"total_score\",\n",
    "                \"content_id\",\n",
    "               ]\n",
    "    data = userdf[DV + features].dropna()\n",
    "    return data\n",
    "\n",
    "def split_data(data): \n",
    "    \"\"\"Split data into X, Y\"\"\"\n",
    "    Y = data[\"engaged\"]\n",
    "    content_ids = data[\"content_id\"].values\n",
    "    features = data[[col for col in data.columns if col not in (\"engaged\", \"content_id\")]]\n",
    "    # dummify anything still in categorical form\n",
    "    X = pd.get_dummies(features)\n",
    "    return X, Y, content_ids\n",
    "\n",
    "def train_and_evaluate(X_feats, Y_feats, clf, print_it=False):\n",
    "    \"\"\"Run cross-validation for crude model evaluation.\"\"\"\n",
    "    X = X_feats.values\n",
    "    Y = Y_feats.values\n",
    "    kf = sklearn.cross_validation.StratifiedKFold(Y, n_folds=2, shuffle=True, random_state=100)\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        if print_it:\n",
    "            print \"WORKING ON FOLD %s:\" %i\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        clf.fit(X_train, Y_train)\n",
    "        predicted = clf.predict(X_test)\n",
    "        probabilities = [p[1] for p in clf.predict_proba(X_test)]\n",
    "        print_metrics(Y_test, predicted, probabilities, print_it=print_it)\n",
    "    return Y_test, predicted, probabilities\n",
    "\n",
    "def print_metrics(true, predicted, proba, print_it=False):\n",
    "    \"\"\"For debugging/investigating individual user metrics.\"\"\"\n",
    "    auc = sklearn.metrics.roc_auc_score(true, proba)\n",
    "    accuracy = sklearn.metrics.accuracy_score(true, predicted)\n",
    "    f1 = sklearn.metrics.f1_score(true, predicted)\n",
    "    recall = sklearn.metrics.recall_score(true, predicted)\n",
    "    precision = sklearn.metrics.precision_score(true, predicted)\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(true, proba)\n",
    "    if print_it:\n",
    "        print \"accuracy: %s\" % accuracy\n",
    "        print \"auc: %s\" % auc\n",
    "        print \"f1: %s\" % f1\n",
    "        print \"recall: %s\" % recall\n",
    "        print \"precision: %s\" % precision\n",
    "        \n",
    "def final_predict(kaid, userdf, clf):\n",
    "    # split into positive and negative examples (all positive examples will go in training data)\n",
    "    positives = userdf[userdf[\"engaged\"]==1]\n",
    "    negatives = userdf[userdf[\"engaged\"]==0]\n",
    "    \n",
    "    # split remaining negatives into training and test (top half most popular will be test)\n",
    "    train = negatives[:len(negatives)/2]\n",
    "    test_ids = negatives[len(negatives)/2:].content_id.values\n",
    "    train_ids = pd.concat([train, positives]).content_id.values\n",
    "    prepped_data = feature_processing(userdf)\n",
    "    \n",
    "    #Split into X and Y + construct training and tests sets\n",
    "    X, Y, cids = split_data(prepped_data)\n",
    "    X_train, Y_train = [], []\n",
    "    X_test, Y_test, test_cids = [], [], []\n",
    "    for i, content_id in enumerate(cids):\n",
    "        if content_id in train_ids:\n",
    "            X_train.append(X.values[i])\n",
    "            Y_train.append(Y.values[i])\n",
    "        elif content_id in test_ids:\n",
    "            X_test.append(X.values[i])\n",
    "            Y_test.append(Y.values[i])\n",
    "            test_cids.append(content_id)\n",
    "            \n",
    "    # fit model and make predictions\n",
    "    clf.fit(X_train, Y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    probas = [p[1] for p in clf.predict_proba(X_test)]\n",
    "    \n",
    "    # construct result structures for storing and using the data\n",
    "    df = pd.DataFrame(data={\n",
    "            \"prediction\": predictions,\n",
    "            \"probability\": probas,\n",
    "            \"content_id\": test_cids\n",
    "        })\n",
    "    r = pd.merge(df, userdf[[\"content_id\", \"node_slug\", \"total_score\", \"kind\", \"domain\", \"subject\"]], how=\"inner\")\n",
    "    r[\"pscore\"] = r[\"probability\"]**.5\n",
    "    r[\"general_zscore\"] = (r[\"total_score\"] - r[\"total_score\"].min())/(r[\"total_score\"].max() - r[\"total_score\"].min())\n",
    "    r[\"score\"] = (r[\"pscore\"] + r[\"general_zscore\"])/2\n",
    "    r = r.sort(\"score\", ascending=False)\n",
    "    r[\"kaid\"] = [kaid for i in range(len(r))]\n",
    "    return r\n",
    "\n",
    "def update_user_summary(userdf, kaid, true, prob, summary):\n",
    "    \"\"\"Update the table of single user summary stats.\"\"\"\n",
    "    summary[\"auc\"].append(sklearn.metrics.roc_auc_score(true, prob))\n",
    "    fpr, tpr, _ = sklearn.metrics.roc_curve(true, prob)\n",
    "    summary[\"num_positive\"].append(userdf.engaged.sum())\n",
    "    summary[\"kaid\"].append(kaid)\n",
    "    summary[\"joined\"].append(userdf.joined.max())\n",
    "    summary[\"fpr\"].append(fpr)\n",
    "    summary[\"tpr\"].append(tpr)\n",
    "    return summary\n",
    "\n",
    "def update_validation(kaid, true, pred, prob, validation):\n",
    "    \"\"\"Update the table of full truth vs. prediction stats\"\"\"\n",
    "    validation[\"true\"].extend(true)\n",
    "    validation[\"predicted\"].extend(pred)\n",
    "    validation[\"probability\"].extend(prob)\n",
    "    validation[\"kaid\"].extend([kaid for i in true])\n",
    "    return validation\n",
    "\n",
    "def classification_pipeline(df, top_n=100):\n",
    "    \"\"\"Run a the full classification pipeline to generate data for model evaluation, \n",
    "       and predictions for the recommender system.\"\"\"\n",
    "    \n",
    "    # initialize resulting table data\n",
    "    results = []\n",
    "    validation = {\"true\":[], \"predicted\":[], \"probability\": [], \"kaid\":[]}\n",
    "    summary = {\"auc\":[], \"num_positive\": [], \"kaid\": [], \"joined\": [], \"fpr\":[], \"tpr\":[]}\n",
    "    \n",
    "    # train, evaluate, and predict for each user individually\n",
    "    for kaid in df.kaid.unique():\n",
    "        userdf = df[df[\"kaid\"]==kaid]\n",
    "        if userdf[\"engaged\"].sum() <= 2:\n",
    "            print \"Skipping %s\" % kaid\n",
    "            continue\n",
    "        print \"Running %s\" % kaid\n",
    "        # preprocess data\n",
    "        prepped_data = feature_processing(userdf)\n",
    "        # train and evaluate model\n",
    "        X, Y, _ = split_data(prepped_data)\n",
    "        clf = sklearn.linear_model.LogisticRegression()\n",
    "        true, pred, prob = train_and_evaluate(X, Y, clf)\n",
    "        summary = update_user_summary(userdf, kaid, true, prob, summary)\n",
    "        validation = update_validation(kaid, true, pred, prob, validation)\n",
    "        # train full model + generate predictions\n",
    "        clf = sklearn.linear_model.LogisticRegression()\n",
    "        r = final_predict(kaid, userdf, clf)\n",
    "        results.append(r[:top_n])\n",
    "        \n",
    "    # construct final pandas dataframes\n",
    "    final_df = pd.concat(results)\n",
    "    validation_df = pd.DataFrame(data=validation)\n",
    "    summary_df = pd.DataFrame(data=summary)\n",
    "    return final_df, validation_df, summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train + Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:96: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping kaid_322962904607449063775472\n",
      "Skipping kaid_972312616504392465407997\n",
      "Skipping kaid_19374345126826952044339\n",
      "Skipping kaid_556022798793514927715493\n",
      "Skipping kaid_73776605891154008780640\n",
      "Skipping kaid_1127696500567694409297156\n",
      "Skipping kaid_244119897602537658027200\n",
      "Skipping kaid_325374501095247410801428\n",
      "Skipping kaid_192151328833642449115110\n",
      "Skipping kaid_84186644625651643679014\n",
      "Skipping kaid_1119520567302279872320347\n",
      "Skipping kaid_285230491758368655281723\n",
      "Skipping kaid_743766538369022350210895\n",
      "Skipping kaid_545540379645660246145069\n",
      "Skipping kaid_940224542961683573027986\n",
      "Skipping kaid_1123648069412520665874207\n",
      "Skipping kaid_772947842742359777683916\n",
      "Skipping kaid_606579190726124429723926\n",
      "Skipping kaid_988944226525128615460921\n",
      "Skipping kaid_290657002422660855950247\n",
      "Skipping kaid_1153336294682633813125123\n",
      "Skipping kaid_998355490805291061907581\n",
      "Skipping kaid_345400575700767057989355\n",
      "Skipping kaid_445865885051156546517178\n",
      "Skipping kaid_911339152645296679773163\n",
      "Skipping kaid_1106386709890039248044024\n",
      "Skipping kaid_667583961928025118712059\n",
      "Skipping kaid_907719073708591576222492\n",
      "Skipping kaid_130005627098908667476885\n",
      "Skipping kaid_11526414680411168424449\n",
      "Skipping kaid_11081676693564563537834\n",
      "Skipping kaid_224532872513214809867097\n",
      "Skipping kaid_789794698556175322518798\n",
      "Skipping kaid_180706567931426476315946\n",
      "Skipping kaid_646404689686985898578969\n",
      "Skipping kaid_1029804019778862851378411\n"
     ]
    }
   ],
   "source": [
    "final_df, validation_df, summary_df = classification_pipeline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.jointplot(final_df.probability, final_df.total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.jointplot(\"probability\", \"true\", validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(validation_df[\"true\"], validation_df[\"probability\"])\n",
    "print sklearn.metrics.roc_auc_score(validation_df[\"true\"], validation_df[\"probability\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=[4,4])\n",
    "plt.scatter(fpr, tpr)\n",
    "plt.ylim([-.02,1.02])\n",
    "plt.xlim([-.02,1.02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = summary_df.auc.mean()\n",
    "se = summary_df.auc.std(dof=0)/np.sqrt(len(summary_df))\n",
    "#plt.bar([0], [m], yerr=[se])\n",
    "print m, se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_df.auc.hist(bins=25, figsize=[3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.jointplot(\"auc\", \"num_positive\", summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=[4,4])\n",
    "for i, row in summary_df[:10].iterrows():\n",
    "    plt.scatter(row[\"fpr\"], row[\"tpr\"])\n",
    "plt.ylim([-.02,1.02])\n",
    "plt.xlim([-.02,1.02])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save predictions back to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for kaid in final_df.kaid.unique():\n",
    "    print kaid\n",
    "    print final_df[final_df[\"kaid\"]==kaid][:10][\"node_slug\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "variables = [\"kaid\", \"kind\", \"content_id\", \"node_slug\", \"domain\", \"subject\", \"probability\", \"prediction\", \"score\"]\n",
    "predictions_to_save = final_df[variables]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
