{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model\n",
    "import sklearn.cross_validation\n",
    "import sklearn.metrics\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "import util.bq_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load up some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET = \"recommendations\"\n",
    "PREDICTION_TABLE = \"predictions\"\n",
    "BUCKET = \"ka_recommendations/emails/\"\n",
    "BUCKET = \"ka_users/amy/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticating with BigQuery and GCS...\n"
     ]
    }
   ],
   "source": [
    "bq, gcs = util.bq_util.get_authed_clients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'amy/rec_training000000000000.csv']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"rec_training_000000000000.csv\"\n",
    "filename = \"rec_training000000000000.csv\"\n",
    "util.bq_util.list_files_in_gcs_bucket(gcs, os.path.join(BUCKET, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "util.bq_util.download_files_from_gcs(gcs, os.path.join(BUCKET, filename))\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define preprocessing + modeling functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bow_a_field(userdf, field):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    tokenizer = TfidfVectorizer()\n",
    "    title_text_feats = tokenizer.fit_transform(userdf[\"title\"].values).todense()\n",
    "    feature_names = tokenizer.get_feature_names()\n",
    "    d = {}\n",
    "    for i, feat in enumerate(feature_names):\n",
    "        d[feat] = [cell[0] for cell in title_text_feats[:,i].tolist()]\n",
    "    subdf = pd.DataFrame(data=d)\n",
    "    subdf = subdf.rename(columns={col: field + \"_\" + col for col in subdf.columns})\n",
    "    udf = userdf.copy()\n",
    "    for col in subdf.columns:\n",
    "        udf[col] = subdf[col].values\n",
    "    return udf\n",
    "\n",
    "def feature_processing(userdf):\n",
    "    \"\"\"Preprocess dataframe for a single user\"\"\"\n",
    "    DV = [\"engaged\"] # DV = did they engage with the content\n",
    "    features = [\"kind\", \n",
    "                \"title\", \n",
    "                \"domain\", \n",
    "                \"subject\", \n",
    "                \"topic\", \n",
    "                #\"keywords\"\n",
    "                \"tutorial\", \n",
    "                \"num_learners_per_day\", \n",
    "                \"total_score\",\n",
    "                \"content_id\",\n",
    "               ]\n",
    "    data = userdf[DV + features].dropna()\n",
    "    for field in [\"title\"]:\n",
    "        data = bow_a_field(data, field)\n",
    "    return data\n",
    "\n",
    "def split_data(data): \n",
    "    \"\"\"Split data into X, Y\"\"\"\n",
    "    Y = data[\"engaged\"]\n",
    "    content_ids = data[\"content_id\"].values\n",
    "    features = data[[col for col in data.columns if col not in (\"engaged\", \"content_id\")]]\n",
    "    # dummify anything still in categorical form\n",
    "    X = pd.get_dummies(features)\n",
    "    return X, Y, content_ids\n",
    "\n",
    "def train_and_evaluate(X_feats, Y_feats, clf, print_it=False):\n",
    "    \"\"\"Run cross-validation for crude model evaluation.\"\"\"\n",
    "    X = X_feats.values\n",
    "    Y = Y_feats.values\n",
    "    kf = sklearn.cross_validation.StratifiedKFold(Y, n_folds=2, shuffle=True, random_state=100)\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        if print_it:\n",
    "            print \"WORKING ON FOLD %s:\" %i\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        clf.fit(X_train, Y_train)\n",
    "        predicted = clf.predict(X_test)\n",
    "        probabilities = [p[1] for p in clf.predict_proba(X_test)]\n",
    "        print_metrics(Y_test, predicted, probabilities, print_it=print_it)\n",
    "    return Y_test, predicted, probabilities\n",
    "\n",
    "def print_metrics(true, predicted, proba, print_it=False):\n",
    "    \"\"\"For debugging/investigating individual user metrics.\"\"\"\n",
    "    auc = sklearn.metrics.roc_auc_score(true, proba)\n",
    "    accuracy = sklearn.metrics.accuracy_score(true, predicted)\n",
    "    f1 = sklearn.metrics.f1_score(true, predicted)\n",
    "    recall = sklearn.metrics.recall_score(true, predicted)\n",
    "    precision = sklearn.metrics.precision_score(true, predicted)\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(true, proba)\n",
    "    if print_it:\n",
    "        print \"accuracy: %s\" % accuracy\n",
    "        print \"auc: %s\" % auc\n",
    "        print \"f1: %s\" % f1\n",
    "        print \"recall: %s\" % recall\n",
    "        print \"precision: %s\" % precision\n",
    "        \n",
    "def final_predict(kaid, userdf, clf):\n",
    "    # split into positive and negative examples (all positive examples will go in training data)\n",
    "    positives = userdf[userdf[\"engaged\"]==1]\n",
    "    negatives = userdf[userdf[\"engaged\"]==0]\n",
    "    \n",
    "    # split remaining negatives into training and test (top half most popular will be test)\n",
    "    train = negatives[:len(negatives)/2]\n",
    "    test_ids = negatives[len(negatives)/2:].content_id.values\n",
    "    train_ids = pd.concat([train, positives]).content_id.values\n",
    "    prepped_data = feature_processing(userdf)\n",
    "    \n",
    "    #Split into X and Y + construct training and tests sets\n",
    "    X, Y, cids = split_data(prepped_data)\n",
    "    X_train, Y_train = [], []\n",
    "    X_test, Y_test, test_cids = [], [], []\n",
    "    for i, content_id in enumerate(cids):\n",
    "        if content_id in train_ids:\n",
    "            X_train.append(X.values[i])\n",
    "            Y_train.append(Y.values[i])\n",
    "        elif content_id in test_ids:\n",
    "            X_test.append(X.values[i])\n",
    "            Y_test.append(Y.values[i])\n",
    "            test_cids.append(content_id)\n",
    "            \n",
    "    # fit model and make predictions\n",
    "    clf.fit(X_train, Y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    probas = [p[1] for p in clf.predict_proba(X_test)]\n",
    "    \n",
    "    # construct result structures for storing and using the data\n",
    "    df = pd.DataFrame(data={\n",
    "            \"prediction\": predictions,\n",
    "            \"probability\": probas,\n",
    "            \"content_id\": test_cids\n",
    "        })\n",
    "    r = pd.merge(df, userdf[[\"content_id\", \"node_slug\", \"total_score\", \"kind\", \"domain\", \"subject\"]], how=\"inner\")\n",
    "    r[\"pscore\"] = r[\"probability\"]**.5\n",
    "    r[\"general_zscore\"] = (r[\"total_score\"] - r[\"total_score\"].min())/(r[\"total_score\"].max() - r[\"total_score\"].min())\n",
    "    r[\"score\"] = (r[\"pscore\"] + r[\"general_zscore\"])/2\n",
    "    r = r.sort(\"score\", ascending=False)\n",
    "    r[\"kaid\"] = [kaid for i in range(len(r))]\n",
    "    return r\n",
    "\n",
    "def update_user_summary(userdf, kaid, true, prob, summary):\n",
    "    \"\"\"Update the table of single user summary stats.\"\"\"\n",
    "    summary[\"auc\"].append(sklearn.metrics.roc_auc_score(true, prob))\n",
    "    fpr, tpr, _ = sklearn.metrics.roc_curve(true, prob)\n",
    "    summary[\"num_positive\"].append(userdf.engaged.sum())\n",
    "    summary[\"kaid\"].append(kaid)\n",
    "    summary[\"joined\"].append(userdf.joined.max())\n",
    "    summary[\"fpr\"].append(fpr)\n",
    "    summary[\"tpr\"].append(tpr)\n",
    "    return summary\n",
    "\n",
    "def update_validation(kaid, true, pred, prob, validation):\n",
    "    \"\"\"Update the table of full truth vs. prediction stats\"\"\"\n",
    "    validation[\"true\"].extend(true)\n",
    "    validation[\"predicted\"].extend(pred)\n",
    "    validation[\"probability\"].extend(prob)\n",
    "    validation[\"kaid\"].extend([kaid for i in true])\n",
    "    return validation\n",
    "\n",
    "def classification_pipeline(df, top_n=100):\n",
    "    \"\"\"Run a the full classification pipeline to generate data for model evaluation, \n",
    "       and predictions for the recommender system.\"\"\"\n",
    "    \n",
    "    # initialize resulting table data\n",
    "    results = []\n",
    "    validation = {\"true\":[], \"predicted\":[], \"probability\": [], \"kaid\":[]}\n",
    "    summary = {\"auc\":[], \"num_positive\": [], \"kaid\": [], \"joined\": [], \"fpr\":[], \"tpr\":[]}\n",
    "    \n",
    "    # train, evaluate, and predict for each user individually\n",
    "    for kaid in df.kaid.unique():\n",
    "        userdf = df[df[\"kaid\"]==kaid]\n",
    "        if userdf[\"engaged\"].sum() <= 2:\n",
    "            print \"Skipping %s\" % kaid\n",
    "            continue\n",
    "        print \"Running %s\" % kaid\n",
    "        # preprocess data\n",
    "        prepped_data = feature_processing(userdf)\n",
    "        # train and evaluate model\n",
    "        X, Y, _ = split_data(prepped_data)\n",
    "        clf = sklearn.linear_model.LogisticRegression()\n",
    "        true, pred, prob = train_and_evaluate(X, Y, clf)\n",
    "        summary = update_user_summary(userdf, kaid, true, prob, summary)\n",
    "        validation = update_validation(kaid, true, pred, prob, validation)\n",
    "        # train full model + generate predictions\n",
    "        clf = sklearn.linear_model.LogisticRegression()\n",
    "        r = final_predict(kaid, userdf, clf)\n",
    "        results.append(r[:top_n])\n",
    "        \n",
    "    # construct final pandas dataframes\n",
    "    final_df = pd.concat(results)\n",
    "    validation_df = pd.DataFrame(data=validation)\n",
    "    summary_df = pd.DataFrame(data=summary)\n",
    "    return final_df, validation_df, summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train + Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_df, validation_df, summary_df = classification_pipeline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot(final_df.probability, final_df.total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=\"true\", y=\"probability\", data=validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(validation_df[\"true\"], validation_df[\"probability\"])\n",
    "print sklearn.metrics.roc_auc_score(validation_df[\"true\"], validation_df[\"probability\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=[4,4])\n",
    "plt.scatter(fpr, tpr)\n",
    "plt.ylim([-.02,1.02])\n",
    "plt.xlim([-.02,1.02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = summary_df.auc.mean()\n",
    "se = summary_df.auc.std(dof=0)/np.sqrt(len(summary_df))\n",
    "#plt.bar([0], [m], yerr=[se])\n",
    "print m, se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary_df.auc.hist(bins=25, figsize=[3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot(\"auc\", \"num_positive\", summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=[4,4])\n",
    "for i, row in summary_df[:10].iterrows():\n",
    "    plt.scatter(row[\"fpr\"], row[\"tpr\"])\n",
    "plt.ylim([-.02,1.02])\n",
    "plt.xlim([-.02,1.02])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save predictions back to BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for kaid in final_df.kaid.unique():\n",
    "    print kaid\n",
    "    print final_df[final_df[\"kaid\"]==kaid][:10][\"node_slug\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "variables = [\"kaid\", \"kind\", \"content_id\", \"node_slug\", \"domain\", \"subject\", \"probability\", \"prediction\", \"score\"]\n",
    "predictions_to_save = final_df[variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import util.secrets\n",
    "util.bq_util.upload_df_to_bq(bq, predictions_to_save, util.secrets.BIGQUERY_PROJECT_ID, DATASET, PREDICTION_TABLE, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(util.bq_util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
